{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of FASHION EVA S6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargav23/CNN-Projects/blob/master/FASHION_STEP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kH16rnZ7wt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3f_Odl-7um",
        "colab_type": "text"
      },
      "source": [
        "## Data Transformations\n",
        "\n",
        "We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtssFUKb-jqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.Resize((28, 28)),\n",
        "                                        #transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                        transforms.Resize((28, 28)),\n",
        "                                        #transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQciFYo2B1mO",
        "colab_type": "text"
      },
      "source": [
        "# Dataset and Creating Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4A84rlfDA23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "train = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
        "                                        download=True, transform=train_transforms)\n",
        "test = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
        "                                       download=True, transform=test_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8OLDR79DrHG",
        "colab_type": "code",
        "outputId": "38505829-f2ae-4eac-9569-7b301f7ff30e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubQL3H6RJL3h",
        "colab_type": "text"
      },
      "source": [
        "# The model\n",
        "Let's start with the model we first saw"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FXQlB9kH1ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook, tnrange\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "dropout_value = 0.07\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.convblock1 = nn.Sequential(\n",
        "          nn.Conv2d(1, 32, 3,padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "        self.convblock2 = nn.Sequential(\n",
        "          nn.Conv2d(32, 32, 3,padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "        self.convblock3 = nn.Sequential(\n",
        "          nn.Conv2d(32, 32, 3,padding=1),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.Dropout(dropout_value)      \n",
        "        )\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.convblock4 = nn.Sequential(\n",
        "\t      nn.Conv2d(32, 64, 3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "        self.convblock5 = nn.Sequential(\n",
        "        nn.Conv2d(64, 64, 3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "     \n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.dconvblock1 = nn.Sequential(\n",
        "\t      nn.Conv2d(64, 128, 3,dilation=2,padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "        self.convblock6 = nn.Sequential(\n",
        "        nn.Conv2d(64, 128, 3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.Dropout(dropout_value)\n",
        "        )\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "        nn.Conv2d(128, 128, 3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.Dropout(dropout_value)      \n",
        "        )\n",
        "        '''\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.convblock8 = nn.Sequential(\n",
        "        nn.Conv2d(64, 128, 1,groups=64,dilation=1,padding=1),\n",
        "        nn.Conv2d(128, 256, kernel_size=(1,1))\n",
        "        \n",
        "        \n",
        "          \n",
        "        )\n",
        "\n",
        "        self.convblock9 = nn.Sequential(\n",
        "        nn.Conv2d(256, 256, 1,groups=256,dilation=1,padding=1),\n",
        "        nn.Conv2d(256, 256, kernel_size=(1,1)),\n",
        "           \n",
        "        )\n",
        "\n",
        "        '''\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=(4,4))\n",
        "        ) # output_size = 1\n",
        "\n",
        "        self.convblock10 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=10, kernel_size=(1, 1), padding=0, bias=False)\n",
        "        ) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.pool2(x)\n",
        "        x1 = self.dconvblock1(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = torch.add(x, x1)\n",
        "\n",
        "        #x = self.pool3(x)\n",
        "        #x = self.convblock8(x)\n",
        "        #x = self.convblock9(x)\n",
        "\n",
        "        x = self.gap(x)        \n",
        "        x = self.convblock10(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3-vp8X9LCWo",
        "colab_type": "text"
      },
      "source": [
        "# Model Params\n",
        "Can't emphasize on how important viewing Model Summary is. \n",
        "Unfortunately, there is no in-built model visualizer, so we have to take external help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skB97zIJQQe",
        "colab_type": "code",
        "outputId": "d2d31cc2-20b2-4fde-d18c-d452bfb155cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "              ReLU-2           [-1, 32, 28, 28]               0\n",
            "       BatchNorm2d-3           [-1, 32, 28, 28]              64\n",
            "           Dropout-4           [-1, 32, 28, 28]               0\n",
            "            Conv2d-5           [-1, 32, 28, 28]           9,248\n",
            "              ReLU-6           [-1, 32, 28, 28]               0\n",
            "       BatchNorm2d-7           [-1, 32, 28, 28]              64\n",
            "           Dropout-8           [-1, 32, 28, 28]               0\n",
            "            Conv2d-9           [-1, 32, 28, 28]           9,248\n",
            "             ReLU-10           [-1, 32, 28, 28]               0\n",
            "      BatchNorm2d-11           [-1, 32, 28, 28]              64\n",
            "          Dropout-12           [-1, 32, 28, 28]               0\n",
            "        MaxPool2d-13           [-1, 32, 14, 14]               0\n",
            "           Conv2d-14           [-1, 64, 14, 14]          18,496\n",
            "             ReLU-15           [-1, 64, 14, 14]               0\n",
            "      BatchNorm2d-16           [-1, 64, 14, 14]             128\n",
            "          Dropout-17           [-1, 64, 14, 14]               0\n",
            "           Conv2d-18           [-1, 64, 14, 14]          36,928\n",
            "             ReLU-19           [-1, 64, 14, 14]               0\n",
            "      BatchNorm2d-20           [-1, 64, 14, 14]             128\n",
            "          Dropout-21           [-1, 64, 14, 14]               0\n",
            "        MaxPool2d-22             [-1, 64, 7, 7]               0\n",
            "           Conv2d-23            [-1, 128, 7, 7]          73,856\n",
            "             ReLU-24            [-1, 128, 7, 7]               0\n",
            "      BatchNorm2d-25            [-1, 128, 7, 7]             256\n",
            "          Dropout-26            [-1, 128, 7, 7]               0\n",
            "           Conv2d-27            [-1, 128, 7, 7]          73,856\n",
            "             ReLU-28            [-1, 128, 7, 7]               0\n",
            "      BatchNorm2d-29            [-1, 128, 7, 7]             256\n",
            "          Dropout-30            [-1, 128, 7, 7]               0\n",
            "           Conv2d-31            [-1, 128, 7, 7]         147,584\n",
            "             ReLU-32            [-1, 128, 7, 7]               0\n",
            "      BatchNorm2d-33            [-1, 128, 7, 7]             256\n",
            "          Dropout-34            [-1, 128, 7, 7]               0\n",
            "        AvgPool2d-35            [-1, 128, 1, 1]               0\n",
            "           Conv2d-36             [-1, 10, 1, 1]           1,280\n",
            "================================================================\n",
            "Total params: 372,032\n",
            "Trainable params: 372,032\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 3.71\n",
            "Params size (MB): 1.42\n",
            "Estimated Total Size (MB): 5.13\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__x_SbrL7z3",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing\n",
        "\n",
        "Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs. \n",
        "\n",
        "Let's write train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbkF2nN_LYIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    \n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drokW8wWODKq",
        "colab_type": "text"
      },
      "source": [
        "# Let's Train and test our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMCFxeAKOB53",
        "colab_type": "code",
        "outputId": "e98217fb-4faf-4b13-e832-8d07c78bacfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.20304594933986664 Batch_id=468 Accuracy=83.89: 100%|██████████| 469/469 [00:15<00:00, 30.73it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.3745, Accuracy: 8650/10000 (86.50%)\n",
            "\n",
            "EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.2839033901691437 Batch_id=468 Accuracy=89.84: 100%|██████████| 469/469 [00:15<00:00, 31.11it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2863, Accuracy: 8994/10000 (89.94%)\n",
            "\n",
            "EPOCH: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.19849742949008942 Batch_id=468 Accuracy=91.17: 100%|██████████| 469/469 [00:15<00:00, 30.96it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2457, Accuracy: 9128/10000 (91.28%)\n",
            "\n",
            "EPOCH: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.1420770287513733 Batch_id=468 Accuracy=92.01: 100%|██████████| 469/469 [00:15<00:00, 31.05it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2342, Accuracy: 9160/10000 (91.60%)\n",
            "\n",
            "EPOCH: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.191675066947937 Batch_id=468 Accuracy=92.61: 100%|██████████| 469/469 [00:15<00:00, 31.07it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2151, Accuracy: 9234/10000 (92.34%)\n",
            "\n",
            "EPOCH: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.2210218906402588 Batch_id=468 Accuracy=93.06: 100%|██████████| 469/469 [00:15<00:00, 30.48it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2297, Accuracy: 9186/10000 (91.86%)\n",
            "\n",
            "EPOCH: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.14812050759792328 Batch_id=468 Accuracy=94.36: 100%|██████████| 469/469 [00:15<00:00, 30.73it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2097, Accuracy: 9264/10000 (92.64%)\n",
            "\n",
            "EPOCH: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.22491736710071564 Batch_id=468 Accuracy=94.53: 100%|██████████| 469/469 [00:14<00:00, 31.37it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2045, Accuracy: 9284/10000 (92.84%)\n",
            "\n",
            "EPOCH: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.15381862223148346 Batch_id=468 Accuracy=94.67: 100%|██████████| 469/469 [00:15<00:00, 31.07it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2010, Accuracy: 9308/10000 (93.08%)\n",
            "\n",
            "EPOCH: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.13379299640655518 Batch_id=468 Accuracy=94.89: 100%|██████████| 469/469 [00:15<00:00, 31.01it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2049, Accuracy: 9299/10000 (92.99%)\n",
            "\n",
            "EPOCH: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.1267210692167282 Batch_id=468 Accuracy=94.84: 100%|██████████| 469/469 [00:15<00:00, 30.28it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2034, Accuracy: 9299/10000 (92.99%)\n",
            "\n",
            "EPOCH: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.09608342498540878 Batch_id=468 Accuracy=95.02: 100%|██████████| 469/469 [00:15<00:00, 30.46it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1997, Accuracy: 9314/10000 (93.14%)\n",
            "\n",
            "EPOCH: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.1491801142692566 Batch_id=468 Accuracy=95.18: 100%|██████████| 469/469 [00:15<00:00, 30.78it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2023, Accuracy: 9313/10000 (93.13%)\n",
            "\n",
            "EPOCH: 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.09793169051408768 Batch_id=468 Accuracy=95.17: 100%|██████████| 469/469 [00:15<00:00, 30.54it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1998, Accuracy: 9327/10000 (93.27%)\n",
            "\n",
            "EPOCH: 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.17458029091358185 Batch_id=468 Accuracy=95.20: 100%|██████████| 469/469 [00:15<00:00, 30.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2006, Accuracy: 9323/10000 (93.23%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odozjbIvY12p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}